{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Ozan Ilhan, s3559513; Volodymyr Kalinin, s3648214"
      ],
      "metadata": {
        "id": "-UaUfYAhs4Dm"
      },
      "id": "-UaUfYAhs4Dm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2601675e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2601675e",
        "outputId": "f27e23bc-576e-4766-8226-454214432a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: svgling in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: svgwrite in /usr/local/lib/python3.10/dist-packages (from svgling) (1.4.3)\n"
          ]
        }
      ],
      "source": [
        "# Install NLTK\n",
        "!python -m pip install numpy nltk svgling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43f1b4fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43f1b4fc",
        "outputId": "9953e294-45cf-491d-bb48-23dad63dcd0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Download the Brown Corpus\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "\n",
        "# Imports\n",
        "from typing import List, Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d016672",
      "metadata": {
        "id": "4d016672"
      },
      "source": [
        "# Natural Language Processing Assignment 1: The Notebook\n",
        "\n",
        "This is the notebook for the first hand-in assignment for Natural Language Processing. The notebook counts for 50% of the total assignment, which counts towards 15% of the final grade."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5dd44fb",
      "metadata": {
        "id": "d5dd44fb"
      },
      "source": [
        "## Assignment 5 (20 points)\n",
        "\n",
        "In the exercises you had the opportunity to practice with fitting $n$-gram language models on the Brown corpus. In this assignment you will fit an interpolated model and learn how to avoid zero counts by exploiting a heldout set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5fa788e",
      "metadata": {
        "id": "c5fa788e"
      },
      "outputs": [],
      "source": [
        "# Imports + code from exercises\n",
        "from nltk.corpus import brown\n",
        "import random\n",
        "from nltk.lm import MLE, Laplace\n",
        "from nltk.lm.api import LanguageModel\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "def split_data(data, train_ratio: float, heldout_ratio: float):\n",
        "    random.Random(42).shuffle(data)\n",
        "    train_cutoff = int(train_ratio*len(data))\n",
        "    heldout_cutoff = int((train_ratio+heldout_ratio)*len(data))\n",
        "    return {'train': data[:train_cutoff], 'heldout': data[train_cutoff:heldout_cutoff], 'test': data[heldout_cutoff:]}\n",
        "\n",
        "# data: a list of (unpadded) sentences that are split into words\n",
        "# n: the maximum order of n-grams to include\n",
        "# model_type: parameter to support different types of language models\n",
        "def fit_language_model(data: List[List[str]], n: int, model_type=MLE):\n",
        "    train, vocab = padded_everygram_pipeline(n, data)\n",
        "    model = model_type(n)\n",
        "    model.fit(train, vocab)\n",
        "    return model\n",
        "\n",
        "def evaluate_language_model(model: LanguageModel, test_data: List[str], n: int):\n",
        "    # Note that `padded_everygram_pipeline' adds n-1 SOS and EOS symbols\n",
        "    # which trickles through in calculating perplexity.\n",
        "    # For example, padding for n=3 will add the bigram for (<s>, <s>) which is NOT present in the model for n=2.\n",
        "    gram_data, _ = padded_everygram_pipeline(n, test_data)\n",
        "    test_grams = [x for t in gram_data for x in model.vocab.lookup(t) if len(x)==n]\n",
        "    return model.perplexity(test_grams)\n",
        "\n",
        "# Prepare data\n",
        "non_fiction = brown.sents(categories=['editorial', 'government', 'learned', 'news', 'reviews'])\n",
        "non_fiction = [s for s in non_fiction if len(s)>=3]\n",
        "\n",
        "fiction = brown.sents(categories=['adventure', 'fiction', 'lore', 'mystery', 'romance', 'science_fiction'])\n",
        "fiction = [s for s in fiction if len(s)>=3]\n",
        "\n",
        "fiction_data = split_data(fiction, 0.9, 0.05)\n",
        "non_fiction_data = split_data(non_fiction, 0.9, 0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "110ce005",
      "metadata": {
        "id": "110ce005"
      },
      "source": [
        "### Part 5.1: training a language model (10 points)\n",
        "\n",
        "You may recall from the exercises that a simple smoothed language model doesn't perform so well on a test set, and generates strange examples. In this assignment you will investigate the use of an interpolated language model, combining it with parameters extracted from a heldout set.\n",
        "\n",
        "Given are the following data: like in the exercises, we work with the Brown corpus and specifically the fiction and non-fiction texts of the corpus. Moreover, we will fix all data to contain all $n$-grams up to $n=3$.\n",
        "\n",
        "As a first step, make sure you have fiction and non-fiction models available with Laplace smoothing (you may have fit these in the exercises already). Then, fit similar models but now use the `AbsoluteDiscountingInterpolated` model type, that implements absolute discounting. Calculate the perplexity to realize that once again there will be zero counts because the model is unsmoothed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a475a4fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a475a4fd",
        "outputId": "a8a9aab0-2d6c-489a-c61e-61b5aee63207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The perplexity of the Laplace model on the fiction test data is: 8015.362978808722\n",
            "The perplexity of the Laplace model on the non-fiction test data is: 13087.796641479064\n",
            "The perplexity of the interpolated model on the fiction test data is: inf\n",
            "The perplexity of the interpolated model on the non-fiction test data is: inf\n"
          ]
        }
      ],
      "source": [
        "# Part 5.1 SOLUTION:\n",
        "from nltk.lm import AbsoluteDiscountingInterpolated\n",
        "\n",
        "n = 3\n",
        "fiction_laplace_model = fit_language_model(fiction_data['train'], n, Laplace)\n",
        "non_fiction_laplace_model = fit_language_model(non_fiction_data['train'], n, Laplace)\n",
        "\n",
        "fiction_discounting_model = fit_language_model(fiction_data['train'], n, AbsoluteDiscountingInterpolated)\n",
        "non_fiction_discounting_model = fit_language_model(non_fiction_data['train'], n, AbsoluteDiscountingInterpolated)\n",
        "\n",
        "fiction_laplace_perplexity = evaluate_language_model(fiction_laplace_model, fiction_data['test'], n)\n",
        "non_fiction_laplace_perplexity = evaluate_language_model(non_fiction_laplace_model, non_fiction_data['test'], n)\n",
        "\n",
        "fiction_discounting_perplexity = evaluate_language_model(fiction_discounting_model, fiction_data['test'], n)\n",
        "non_fiction_discounting_perplexity = evaluate_language_model(non_fiction_discounting_model, non_fiction_data['test'], n)\n",
        "\n",
        "print(f\"The perplexity of the Laplace model on the fiction test data is: {fiction_laplace_perplexity}\")\n",
        "print(f\"The perplexity of the Laplace model on the non-fiction test data is: {non_fiction_laplace_perplexity}\")\n",
        "print(f\"The perplexity of the interpolated model on the fiction test data is: {fiction_discounting_perplexity}\")\n",
        "print(f\"The perplexity of the interpolated model on the non-fiction test data is: {non_fiction_discounting_perplexity}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41afef4f",
      "metadata": {
        "id": "41afef4f"
      },
      "source": [
        "### Part 5.2: avoiding zero counts (10 points)\n",
        "\n",
        "One way of avoiding zero counts is to make sure that unseen words are mapped to a special token `<UNK>` (unknown). But how do we quantify the $n$-gram probability of unknown words? Well, we can use the held out set to estimate these counts. The implementation of language models in NLTK then allows us to simply *update* a language model with more data.\n",
        "\n",
        "Hence, your assignment here is to implement the method `update_model` below. It takes three inputs: a language model, the heldout data, and the parameter `n` for the maximum $n$-grams to include. What the method should do:\n",
        "\n",
        " 1. Use the language model's vocabulary to map unseen words of the heldout data to the `<UNK>` token,\n",
        " 2. Generate the $n$-grams for the (tokenized) heldout data,\n",
        " 3. Update the language model with these counts\n",
        "\n",
        "After successful implementation of the method, use it to update the interpolated model and calculate its perplexity. How does it compare to the perplexity of the Laplace smoothed model?\n",
        "\n",
        "*Hint: you may take inspiration from the method `evaluate_language_model` above.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ac22c4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ac22c4d",
        "outputId": "e3feca6e-88ce-46c5-b580-eb987ca1dab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The perplexity of the updated interpolated model on the fiction test data is: 267.0908535777575, the old perplexity was: inf.\n",
            "The perplexity of the updated interpolated model on the non-fiction test data is: 472.7387360379157, the old perplexity was: inf.\n"
          ]
        }
      ],
      "source": [
        "# Part 5.2 SOLUTION:\n",
        "\n",
        "def update_model(model: LanguageModel, heldout_data: List[List[str]], n: int):\n",
        "    updated_heldout_data = []\n",
        "    # go through the heldout data and map unseen words to <UNK>\n",
        "    for sentence in heldout_data:\n",
        "        for word in sentence:\n",
        "            if word in model.vocab:\n",
        "                updated_heldout_data.append(word)\n",
        "            else:\n",
        "                updated_heldout_data.append('<UNK>')\n",
        "    heldout, _ = padded_everygram_pipeline(n, updated_heldout_data)\n",
        "\n",
        "    # use the model.vocab since because we are mapping unseen words to the vocabulary of the model\n",
        "    model.fit(heldout, model.vocab)\n",
        "\n",
        "update_model(fiction_discounting_model, fiction_data['heldout'], n)\n",
        "update_model(non_fiction_discounting_model, non_fiction_data['heldout'], n)\n",
        "\n",
        "updated_fiction_discounting_perplexity = evaluate_language_model(fiction_discounting_model, fiction_data['test'], n)\n",
        "updated_non_fiction_discounting_perplexity = evaluate_language_model(non_fiction_discounting_model, fiction_data['test'], n)\n",
        "\n",
        "print(f\"The perplexity of the updated interpolated model on the fiction test data is: {updated_fiction_discounting_perplexity}, the old perplexity was: {fiction_discounting_perplexity}.\")\n",
        "print(f\"The perplexity of the updated interpolated model on the non-fiction test data is: {updated_non_fiction_discounting_perplexity}, the old perplexity was: {non_fiction_discounting_perplexity}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46e9b8b1",
      "metadata": {
        "id": "46e9b8b1"
      },
      "source": [
        "## Assignment 6: part-of-speech tagging (20 points)\n",
        "\n",
        "This assignment consists of two parts in which you will consider parts-of-speech (POS) and POS tagging."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c682096b",
      "metadata": {
        "id": "c682096b"
      },
      "source": [
        "### Part 6.1: Ambiguity in open and closed class words (10 points)\n",
        "\n",
        "In the lecture and in the exercises you saw that a small portion of words is ambiguous, but they nevertheless cover a large portion of text corpora. You also learnt about the distinction between open (i.e. dynamic words that may appear and disappear over time) and closed class words (i.e. pretty much fixed sets of words).\n",
        "\n",
        "Below are two lists: one corresponds to closed class words, and another corresponds to open class words. We wish to inspect the ambiguity of words that are open class and words that are open class, and figure out whether words can belong to both!\n",
        "\n",
        "Your task is as follows:\n",
        "1. Determine the percentage of *unique* words that occur both as open class and as closed class words. I.e. out of all *unique* words, how many of them have been tagged at least once as open and and at least once as closed class?\n",
        "2. For the *unique* words that only occur as either open class words or closed class words, calculate the percentage of *unique* words in each class that is may receive multiple POS tags (in their respective class). Do you find that open class words are more ambiguous, or less ambiguous then closed class words?\n",
        "\n",
        "*Hint: If you run into any performance issues because of iterating over large lists, remember that you can convert a list into a set first and then membership lookup in Python becomes linear.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce6fc9dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce6fc9dc",
        "outputId": "001990d3-6a18-4759-c722-a6904f7dfd5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import treebank\n",
        "nltk.download('treebank') # i had to add this because it was giving an error\n",
        "\n",
        "all_word_tags = [word_tag for tagged_sentence in treebank.tagged_sents() for word_tag in tagged_sentence]\n",
        "closed_class_tags = [',', '$', '.', 'CD', 'CC', 'RP', \"''\", '``', 'IN', 'TO',':', 'DT', 'WP', 'PRP',\n",
        "                     'SYM', 'EX', 'PRP$', 'POS', 'MD']\n",
        "open_class_tags = ['JJ', 'JJR', 'JJS', 'VBG', 'VBN', 'VB', 'NNP', 'VBD', 'NN', 'NNS',  'VBZ', 'VBP', 'NNPS']\n",
        "other_tags = ['RBR', '-LRB-', 'PDT', 'WRB', 'WDT', 'RB', 'LS', 'WP$',  'UH',\n",
        "              'FW', '-RRB-',  'RBS', '#', '-NONE-']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdb5133e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdb5133e",
        "outputId": "54db5594-f7d7-493f-e2bf-22122116dddc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of unique words that are both open class and closed class words: 0.38%\n",
            "Percentage of ambiguous open class words: 10.1%\n",
            "Percentage of ambiguous closed class words: 2.1%\n",
            "\n",
            "The open class words are more ambigious than the closed class words, since these words are more likely to have multiple tags.\n"
          ]
        }
      ],
      "source": [
        "# 6.1 SOLUTION:\n",
        "\n",
        "# find which words belongs to the open class and closed class\n",
        "all_word_tags = set(all_word_tags)\n",
        "open_class_words = set()\n",
        "closed_class_words = set()\n",
        "for word, tag in all_word_tags:\n",
        "    if tag in open_class_tags:\n",
        "        open_class_words.add(word)\n",
        "    if tag in closed_class_tags:\n",
        "        closed_class_words.add(word)\n",
        "\n",
        "# find the percentage of unique words that are both open class and closed class words\n",
        "words_in_both_classes = open_class_words & closed_class_words\n",
        "total_unique_words = len(open_class_words.union(closed_class_words))\n",
        "both_classes_percentage = len(words_in_both_classes) / total_unique_words * 100\n",
        "print(f\"Percentage of unique words that are both open class and closed class words: {round(both_classes_percentage, 2)}%\")\n",
        "\n",
        "open_class_dict = dict()\n",
        "closed_class_dict = dict()\n",
        "\n",
        "# map each word to its tags in a dictionary for both open and closed class words\n",
        "for word, tag in all_word_tags:\n",
        "    if tag in open_class_tags:\n",
        "        if word in open_class_dict:\n",
        "            open_class_dict[word].add(tag)\n",
        "        else:\n",
        "            open_class_dict[word] = {tag} # a set is needed here to handle removing duplicate tags\n",
        "    if tag in closed_class_tags:\n",
        "        if word in closed_class_dict:\n",
        "            closed_class_dict[word].add(tag)\n",
        "        else:\n",
        "            closed_class_dict[word] = {tag}\n",
        "\n",
        "# find the percentage of ambiguous words in the open and closed class by looking at the dictionary for words with multiple tags\n",
        "ambiguous_open_class_words = []\n",
        "for word, tag in open_class_dict.items():\n",
        "    if len(tag) > 1:\n",
        "        ambiguous_open_class_words.append(word)\n",
        "\n",
        "amibigious_closed_class_words = []\n",
        "for word, tag in closed_class_dict.items():\n",
        "    if len(tag) > 1:\n",
        "        amibigious_closed_class_words.append(word)\n",
        "open_class_percentage = len(ambiguous_open_class_words) / len(open_class_dict) * 100\n",
        "closed_class_percentage = len(amibigious_closed_class_words) / len(closed_class_dict) * 100\n",
        "\n",
        "print(f\"Percentage of ambiguous open class words: {round(open_class_percentage, 1)}%\")\n",
        "print(f\"Percentage of ambiguous closed class words: {round(closed_class_percentage, 1)}%\")\n",
        "print()\n",
        "print(\"The open class words are more ambigious than the closed class words, since these words are more likely to have multiple tags.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74abe745",
      "metadata": {
        "id": "74abe745"
      },
      "source": [
        "### Part 6.2: Smoothing a tagger (10 points)\n",
        "\n",
        "In the exercises you trained an HMM tagger that used `LidstoneProbDist` as an estimator for its probability distributions. This estimator generally applies some form of smoothing for its probability calculation. In general this would mean that the probability of a transition from one tag to the other is defined as:\n",
        "\n",
        "$$ P_{\\beta}(t_i | t_{i-1}) = \\frac{C(t_{i-1} t_i) + \\beta}{C(t_{i-1}) + \\beta \\cdot V} $$\n",
        "        \n",
        "where $V$ is the number of possible tags. A similar smoothing applies to the calculation of emission probabilities $P(w_i | t_i)$.\n",
        "\n",
        "In the exercises the smoothing parameter was set as $\\beta=0.1$.\n",
        "\n",
        "In this assignment, you will try to find a better parameter on a given dataset. The code below loads in a portion of the Penn Treebank and has code for training and testing a Hidden Markov Model tagger.\n",
        "\n",
        "Your task is the following:\n",
        "\n",
        "1. Train a tagger with smoothing parameter $0.1$ and verify that its accuracy is $0.917$,\n",
        "2. Train an unsmoothed tagger, i.e. the smoothing parameter should be set to $0$, and report its accuracy,\n",
        "3. Now find a smoothing value for which the corresponding model reaches an accuracy greater than $0.917$. Is it higher or lower than $0.1$?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7e47e49",
      "metadata": {
        "id": "e7e47e49"
      },
      "outputs": [],
      "source": [
        "from nltk import LidstoneProbDist\n",
        "from nltk.tag.hmm import load_pos, HiddenMarkovModelTrainer, HiddenMarkovModelTagger\n",
        "from nltk.lm import MLE, Laplace\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_hmm_tagger(train_data: List[List[Tuple[str, str]]],\n",
        "                     estimator=lambda fd, bins: LidstoneProbDist(fd, 0.1, bins)) -> HiddenMarkovModelTagger:\n",
        "    tags = list(set([t for s in train_data for w,t in s]))\n",
        "    words = list(set([w for s in train_data for w,t in s]))\n",
        "    trainer = HiddenMarkovModelTrainer(tags, words)\n",
        "    hmm = trainer.train_supervised(train_data, estimator=estimator)\n",
        "    return hmm\n",
        "\n",
        "def test_hmm_tagger(tagger: HiddenMarkovModelTagger, test_data: List[List[Tuple[str, str]]]) -> List[List[Tuple[str, str]]]:\n",
        "    # we put disable=True to avoid the progress bar in the output\n",
        "    return [tagger.tag([w for w,t in s]) for s in tqdm(test, position=0, disable=True)]\n",
        "\n",
        "def accuracy(trues: List[str], predictions: List[str]):\n",
        "    return sum([t == p for t, p in zip(trues, predictions)])/len(trues)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8da75bf3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "8da75bf3",
        "outputId": "1d2adc0e-c487-4945-b0e8-f710b576615c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "3522"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "392"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from nltk.corpus import treebank\n",
        "\n",
        "import random\n",
        "\n",
        "def split_data(data, cutoff=0.98):\n",
        "    random.Random(42).shuffle(data)\n",
        "    return data[:int(cutoff*len(data))], data[int(cutoff*len(data)):]\n",
        "\n",
        "train, test = split_data(list(treebank.tagged_sents()), cutoff=0.9)\n",
        "display(len(train), len(test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57d8d99f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57d8d99f",
        "outputId": "b3d401ab-5431-485d-9e5f-3516c1e5bbdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with beta = 0.1: 0.917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:334: RuntimeWarning: overflow encountered in cast\n",
            "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:332: RuntimeWarning: overflow encountered in cast\n",
            "  P[i] = self._priors.logprob(si)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:364: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with beta = 0: 0.4079\n",
            "Accuracy with beta = 0.001: 0.9192\n",
            "Accuracy with beta = 0.005: 0.9213\n",
            "Accuracy with beta = 0.01: 0.9221\n",
            "Accuracy with beta = 0.05: 0.9213\n",
            "Accuracy with beta = 0.1: 0.917\n",
            "Accuracy with beta = 0.15: 0.9137\n",
            "Accuracy with beta = 0.2: 0.912\n",
            "Accuracy with beta = 0.5: 0.8878\n",
            "Accuracy with beta = 1: 0.8597\n",
            "The best smoothing value is: 0.01, which gives an accuracy of: 0.9221. This is higher than 0.917.\n"
          ]
        }
      ],
      "source": [
        "# 6.2 SOLUTION:\n",
        "hmm_tagger_0_1 = train_hmm_tagger(train)\n",
        "predictions_0_1 = test_hmm_tagger(hmm_tagger_0_1, test)\n",
        "test_tags = [tag for sentence in test for (word, tag) in sentence]\n",
        "predicted_tags_0_1 = [tag for sentence in predictions_0_1 for (word, tag) in sentence]\n",
        "accuracy_0_1 = accuracy(test_tags, predicted_tags_0_1)\n",
        "print(f\"Accuracy with beta = 0.1: {round(accuracy_0_1, 3)}\")\n",
        "\n",
        "hmm_tagger_0 = train_hmm_tagger(train, estimator=lambda fd, bins: LidstoneProbDist(fd, 0, bins))\n",
        "predictions_0 = test_hmm_tagger(hmm_tagger_0, test)\n",
        "predicted_tags_0 = [tag for sentence in predictions_0 for (word, tag) in sentence]\n",
        "accuracy_0 = accuracy(test_tags, predicted_tags_0)\n",
        "print(f\"Accuracy with beta = 0: {round(accuracy_0, 4)}\")\n",
        "\n",
        "beta_values  = [0.001, 0.005, 0.01, 0.05, 0.1, 0.15, 0.2, 0.5, 1]\n",
        "accuracies = []\n",
        "for beta in beta_values:\n",
        "    hmm_tagger = train_hmm_tagger(train, estimator=lambda fd, bins: LidstoneProbDist(fd, beta, bins))\n",
        "    predictions = test_hmm_tagger(hmm_tagger, test)\n",
        "    predicted_tags = [tag for sentence in predictions for (word, tag) in sentence]\n",
        "    accuracy_tagger = accuracy(test_tags, predicted_tags)\n",
        "    print(f\"Accuracy with beta = {beta}: {round(accuracy_tagger, 4)}\")\n",
        "    accuracies.append((beta, accuracy_tagger))\n",
        "\n",
        "best_beta, best_accuracy = max(accuracies, key=lambda x: x[1]) # find the beta value with the highest accuracy\n",
        "\n",
        "print(f\"The best smoothing value is: {best_beta}, which gives an accuracy of: {round(best_accuracy, 4)}. This is higher than 0.917.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1275c398",
      "metadata": {
        "id": "1275c398"
      },
      "source": [
        "## Assignment 7: parsing (20 points)\n",
        "\n",
        "In this assignment, you will perform evaluation over a constituency parser. The first bit of code is taken from the exercises and sets up a probabilistic context-free grammar and a Viterbi parser for it, over a sample from the Penn Treebank."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dafb64a",
      "metadata": {
        "id": "4dafb64a"
      },
      "outputs": [],
      "source": [
        "from nltk.tree.tree import Tree\n",
        "from nltk.corpus import treebank\n",
        "from nltk.grammar import Nonterminal, induce_pcfg\n",
        "from nltk.parse.viterbi import ViterbiParser\n",
        "\n",
        "parsed_sents = treebank.parsed_sents()\n",
        "grammar = induce_pcfg(Nonterminal(\"S\"), [p for s in parsed_sents for p in s.productions()])\n",
        "viterbi_parser = ViterbiParser(grammar)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23c3938c",
      "metadata": {
        "id": "23c3938c"
      },
      "source": [
        "The code below loads and displays two example test trees (for the sentences `They will join the board .` and `The student will do the work on Monday .`) containing a few parsing errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73ea9bf8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73ea9bf8",
        "outputId": "9960fd0b-b65d-4ce6-eed6-138565879499"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('NP-SBJ', [Tree('PRP', ['They'])]), Tree('VP', [Tree('VB', ['will']), Tree('NP', [Tree('VB', ['join']), Tree('NP', [Tree('DT', ['the']), Tree('NN', ['board'])])])]), Tree('.', ['.'])])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"264px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,280.0,264.0\" width=\"280px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"22.8571%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP-SBJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">They</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.4286%\" y1=\"20px\" y2=\"48px\" /><svg width=\"68.5714%\" x=\"22.8571%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VP</text></svg><svg width=\"25%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">will</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.5%\" y1=\"20px\" y2=\"48px\" /><svg width=\"75%\" x=\"25%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"33.3333%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">join</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.6667%\" y1=\"20px\" y2=\"48px\" /><svg width=\"66.6667%\" x=\"33.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"41.6667%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"20.8333%\" y1=\"20px\" y2=\"48px\" /><svg width=\"58.3333%\" x=\"41.6667%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">board</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"70.8333%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66.6667%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"62.5%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"57.1429%\" y1=\"20px\" y2=\"48px\" /><svg width=\"8.57143%\" x=\"91.4286%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"95.7143%\" y1=\"20px\" y2=\"48px\" /></svg>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('NP-SBJ', [Tree('DT', ['The']), Tree('NN', ['student'])]), Tree('VP', [Tree('VB', ['will']), Tree('VP', [Tree('VB', ['do']), Tree('NP', [Tree('DT', ['the']), Tree('NN', ['work']), Tree('PP-CLR', [Tree('IN', ['on']), Tree('NNP', ['Monday'])])])])]), Tree('.', ['.'])])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"312px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,400.0,312.0\" width=\"400px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"28%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP-SBJ</text></svg><svg width=\"35.7143%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">The</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"17.8571%\" y1=\"20px\" y2=\"48px\" /><svg width=\"64.2857%\" x=\"35.7143%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">student</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"67.8571%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14%\" y1=\"20px\" y2=\"48px\" /><svg width=\"66%\" x=\"28%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VP</text></svg><svg width=\"18.1818%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">will</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"9.09091%\" y1=\"20px\" y2=\"48px\" /><svg width=\"81.8182%\" x=\"18.1818%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VP</text></svg><svg width=\"14.8148%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">do</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"7.40741%\" y1=\"20px\" y2=\"48px\" /><svg width=\"85.1852%\" x=\"14.8148%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"21.7391%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"10.8696%\" y1=\"20px\" y2=\"48px\" /><svg width=\"26.087%\" x=\"21.7391%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">work</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"34.7826%\" y1=\"20px\" y2=\"48px\" /><svg width=\"52.1739%\" x=\"47.8261%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PP-CLR</text></svg><svg width=\"33.3333%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">on</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.6667%\" y1=\"20px\" y2=\"48px\" /><svg width=\"66.6667%\" x=\"33.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Monday</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66.6667%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"73.913%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"57.4074%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"59.0909%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"61%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6%\" x=\"94%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"97%\" y1=\"20px\" y2=\"48px\" /></svg>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "with open('example_trees.txt', 'r') as in_file:\n",
        "    example_trees = [Tree.fromstring(s.strip()) for s in in_file.readlines()]\n",
        "\n",
        "display(example_trees[0])\n",
        "display(example_trees[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01058f7f",
      "metadata": {
        "id": "01058f7f"
      },
      "source": [
        "The code below will compute the span representation for a given tree, as a first step in calculating parser evaluation metrics (precision, recall, F1).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d63e6a7d",
      "metadata": {
        "id": "d63e6a7d"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, List\n",
        "\n",
        "def get_leaf_position_for_idx(tree, idx):\n",
        "    return [i for i in range(len(tree.leaves())) if tree.leaf_treeposition(i) == idx][0]\n",
        "\n",
        "def get_span_for_index(tree: Tree, idx: Tuple[int,...]):\n",
        "    return [get_leaf_position_for_idx(tree, p) for p in tree.treepositions()\n",
        "            if p[:len(idx)] == idx and isinstance(tree[p], str)]\n",
        "\n",
        "def get_internal_node_idxs(tree: Tree) -> List[Tuple[int,...]]:\n",
        "    return [i for i in tree.treepositions() if isinstance(tree[i], Tree) and tree[i].height() > 1]\n",
        "\n",
        "def compute_labeled_span_for_node(tree: Tree, idx: Tuple[int,...]) -> Tuple[str, int, int]:\n",
        "    label = tree[idx].label()\n",
        "    span = get_span_for_index(tree, idx)\n",
        "    return label, span[0], span[-1]\n",
        "\n",
        "def string_for_lspan(label: str, left: int, right: int):\n",
        "    return f\"{label}({left},{right})\"\n",
        "\n",
        "def get_labelled_spans_for_tree(tree: Tree) -> List[Tuple[str, int, int]]:\n",
        "    # First grab the tree indices for internal nodes\n",
        "    internal_node_indices = get_internal_node_idxs(tree)\n",
        "    # Compute the labelled spans for all internal nodes\n",
        "    labelled_spans = [compute_labeled_span_for_node(tree, idx) for idx in internal_node_indices]\n",
        "    return [string_for_lspan(label, left, right) for label, left, right in labelled_spans]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82213252",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82213252",
        "outputId": "59bfd70a-d877-49ec-9f3e-74af5c9aa84e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['S(0,5)',\n",
              " 'NP-SBJ(0,0)',\n",
              " 'PRP(0,0)',\n",
              " 'VP(1,4)',\n",
              " 'VB(1,1)',\n",
              " 'NP(2,4)',\n",
              " 'VB(2,2)',\n",
              " 'NP(3,4)',\n",
              " 'DT(3,3)',\n",
              " 'NN(4,4)',\n",
              " '.(5,5)']"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['S(0,8)',\n",
              " 'NP-SBJ(0,1)',\n",
              " 'DT(0,0)',\n",
              " 'NN(1,1)',\n",
              " 'VP(2,7)',\n",
              " 'VB(2,2)',\n",
              " 'VP(3,7)',\n",
              " 'VB(3,3)',\n",
              " 'NP(4,7)',\n",
              " 'DT(4,4)',\n",
              " 'NN(5,5)',\n",
              " 'PP-CLR(6,7)',\n",
              " 'IN(6,6)',\n",
              " 'NNP(7,7)',\n",
              " '.(8,8)']"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "example_spans1 = get_labelled_spans_for_tree(example_trees[0])\n",
        "example_spans2 = get_labelled_spans_for_tree(example_trees[1])\n",
        "\n",
        "display(example_spans1)\n",
        "display(example_spans2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91215d6c",
      "metadata": {
        "id": "91215d6c"
      },
      "source": [
        "### Part 7.1: Parser evaluation (10 points)\n",
        "\n",
        "Do the following:\n",
        "- Use the Viterbi parser from the previous exercise to extract the parses for these sentences.\n",
        "- Finish the implementation for precision, recall and F1 score and calculate these metrics for the parser output against the two test sentences. Verify that you get the following scores:\n",
        "\n",
        "| Sentence        | Precision   | Recall | F1 |\n",
        "|:------------- |:-------------:|:------:|:--:|\n",
        "| `They will join the board .`      | 0.82 | 0.82 | 0.82 |\n",
        "| `The student will do the work on Monday .`    | 0.87  | 0.81 | 0.84 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f605ae3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f605ae3",
        "outputId": "25de7e31-7d5d-4457-b7cd-dc6d09cddf04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1, Precision: 0.82, Recall: 0.82, F1: 0.82\n",
            "Sentence 2, Precision: 0.87, Recall: 0.81, F1: 0.84\n",
            "These scores are the same scores that were given.\n"
          ]
        }
      ],
      "source": [
        "# 7.1 SOLUTION:\n",
        "parsed_sentence1 = list(viterbi_parser.parse(example_trees[0].leaves()))[0]\n",
        "parsed_sentence2 = list(viterbi_parser.parse(example_trees[1].leaves()))[0]\n",
        "\n",
        "parsed_viterbi1 = get_labelled_spans_for_tree(parsed_sentence1)\n",
        "parsed_viterbi2 = get_labelled_spans_for_tree(parsed_sentence2)\n",
        "\n",
        "def calculate_metrics(example_spans, parsed_viterbi):\n",
        "    example_spans = set(example_spans)\n",
        "    parsed_viterbi = set(parsed_viterbi)\n",
        "    correct_spans = example_spans & parsed_viterbi\n",
        "    n_correct = len(correct_spans)\n",
        "    n_example_spans = len(example_spans)\n",
        "    n_viterbi = len(parsed_viterbi)\n",
        "\n",
        "    if n_example_spans > 0:\n",
        "        precision = n_correct / n_example_spans\n",
        "    else:\n",
        "        precision = 0\n",
        "\n",
        "    if n_viterbi > 0:\n",
        "        recall = n_correct / n_viterbi\n",
        "    else:\n",
        "        recall = 0\n",
        "\n",
        "    if (precision + recall) > 0:\n",
        "        f1 = (2 * precision * recall / (precision + recall))\n",
        "    else:\n",
        "        f1 = 0\n",
        "    return precision, recall, f1\n",
        "\n",
        "precision1, recall1, f1_1 = calculate_metrics(example_spans1, parsed_viterbi1)\n",
        "precision2, recall2, f1_2 = calculate_metrics(example_spans2, parsed_viterbi2)\n",
        "\n",
        "print(f\"Sentence 1, Precision: {round(precision1, 2)}, Recall: {round(recall1, 2)}, F1: {round(f1_1, 2)}\")\n",
        "print(f\"Sentence 2, Precision: {round(precision2, 2)}, Recall: {round(recall2, 2)}, F1: {round(f1_2, 2)}\")\n",
        "print(\"These scores are the same scores that were given.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2d256d8",
      "metadata": {
        "id": "c2d256d8"
      },
      "source": [
        "### Part 7.1b (BONUS, 5 points):\n",
        "\n",
        "This question can lead to bonus points, which may compensate any error you made elsewhere.\n",
        "\n",
        "- What happens to the evaluation metrics if you switch the test tree and the parse tree around?\n",
        "- What happens if you compare the first test tree against the second parsed sentence and vice versa?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0468c19d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0468c19d",
        "outputId": "751d668f-647c-4e2a-81be-a6f6435f5dda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After switching the test tree and the parse tree around:\n",
            "Sentence 1, Precision: 0.82, Recall: 0.82, F1: 0.82\n",
            "Sentence 2, Precision: 0.81, Recall: 0.87, F1: 0.84\n",
            "\n",
            "The precision and recall are switched with each other when you do this,\n",
            "because the predicted and actual spans are switched with each other and therefore also the precision and recall.\n",
            "The F1 score remains the same.\n",
            "\n",
            "Comparing the first test tree against the second parsed sentence and vice versa:\n",
            "Sentence 1 and parsed sentence 2, Precision: 0.0, Recall: 0.0, F1: 0\n",
            "Sentence 2 and parsed sentence 1, Precision: 0.07, Recall: 0.09, F1: 0.08\n",
            "\n",
            "All scores get changed to very low values when you do this,\n",
            "because the predicted spans are nothing like the actual span values anymore.\n",
            "This leads to a lot of mismatches and thus low scores.\n"
          ]
        }
      ],
      "source": [
        "# 7.1b SOLUTION:\n",
        "precision_switched1, recall_switched1, f1_switched1 = calculate_metrics(parsed_viterbi1, example_spans1)\n",
        "precision_switched2, recall_switched2, f1_switched2 = calculate_metrics(parsed_viterbi2, example_spans2)\n",
        "\n",
        "print(\"After switching the test tree and the parse tree around:\")\n",
        "print(f\"Sentence 1, Precision: {round(precision_switched1, 2)}, Recall: {round(recall_switched1, 2)}, F1: {round(f1_switched1, 2)}\")\n",
        "print(f\"Sentence 2, Precision: {round(precision_switched2, 2)}, Recall: {round(recall_switched2, 2)}, F1: {round(f1_switched2, 2)}\")\n",
        "print()\n",
        "print(\"\"\"The precision and recall are switched with each other when you do this,\n",
        "because the predicted and actual spans are switched with each other and therefore also the precision and recall.\n",
        "The F1 score remains the same.\"\"\")\n",
        "print()\n",
        "\n",
        "precision_compare1, recall_compare1, f1_compare1 = calculate_metrics(example_spans1, parsed_viterbi2)\n",
        "precision_compare2, recall_compare2, f1_compare2 = calculate_metrics(example_spans2, parsed_viterbi1)\n",
        "print(\"Comparing the first test tree against the second parsed sentence and vice versa:\")\n",
        "print(f\"Sentence 1 and parsed sentence 2, Precision: {round(precision_compare1, 2)}, Recall: {round(recall_compare1, 2)}, F1: {round(f1_compare1, 2)}\")\n",
        "print(f\"Sentence 2 and parsed sentence 1, Precision: {round(precision_compare2, 2)}, Recall: {round(recall_compare2, 2)}, F1: {round(f1_compare2, 2)}\")\n",
        "print()\n",
        "print(\"\"\"All scores get changed to very low values when you do this,\n",
        "because the predicted spans are nothing like the actual span values anymore.\n",
        "This leads to a lot of mismatches and thus low scores.\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9b3267b",
      "metadata": {
        "id": "d9b3267b"
      },
      "source": [
        "### Part 7.2: Large-Scale evaluation (10 points)\n",
        "\n",
        "Now we must find out a way to evaluate on a larger test set of sentences. This set is given in the file `test_trees.txt`, which is already loaded by the code below. Do the following:\n",
        "\n",
        "1. Parse each of the sentences.\n",
        "2. Calculate the micro-averaged precision, recall and F1 scores. That is, you must first aggregate all predictions and then calculate the overall metrics! This time, the trees in the test set correspond to the *correct parses*.\n",
        "\n",
        "*Note: running over all the test trees may take a while. Although it shouldn't take more than a few minutes at most, it is recommended to use `tqdm` when iterating over the test trees to keep track of the progress, as indicated in the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3230a3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3230a3e",
        "outputId": "3538f534-c598-48e4-d555-5b0e8f8da283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 50/50 [04:15<00:00,  5.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro-averaged Precision: 0.78,\n",
            "Micro-average Recall: 0.81,\n",
            "Micro-averaged F1: 0.79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 7.2 SOLUTION:\n",
        "from tqdm import tqdm\n",
        "\n",
        "with open('test_trees.txt', 'r') as in_file:\n",
        "    test_trees = [Tree.fromstring(s.strip()) for s in in_file.readlines()]\n",
        "\n",
        "all_parsed_spans = []\n",
        "all_tree_spans = []\n",
        "for tree in tqdm(test_trees, position=0):\n",
        "    sentence = tree.leaves()\n",
        "    parsed_sentence = list(viterbi_parser.parse(sentence))[0]\n",
        "    parsed_spans = get_labelled_spans_for_tree(parsed_sentence)\n",
        "    tree_spans = get_labelled_spans_for_tree(tree)\n",
        "    all_parsed_spans.extend(parsed_spans)\n",
        "    all_tree_spans.extend(tree_spans)\n",
        "\n",
        "precision_micro, recall_micro, f1_micro = calculate_metrics(all_tree_spans, all_parsed_spans)\n",
        "\n",
        "print(f\"\"\"Micro-averaged Precision: {round(precision_micro, 2)},\n",
        "Micro-average Recall: {round(recall_micro, 2)},\n",
        "Micro-averaged F1: {round(f1_micro, 2)}\"\"\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
