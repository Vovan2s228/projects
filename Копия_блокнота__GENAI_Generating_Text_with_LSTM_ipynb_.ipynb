{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Text with LSTM Networks\n",
        "\n",
        "In this notebook, you will create character-based models for text generation with LSTM recurrent neural networks.\n",
        "\n",
        "**Note:** LSTM models are slow to train. We will start with a small LSTM network and train it for a short time to get a feel for how they work before creating a slightly more complex model and training it for longer to obtain better results.\n"
      ],
      "metadata": {
        "id": "3CF5IQUgagry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Gutenberg\n",
        "\n",
        "Many of the classical texts are no longer protected under copyright. This means you can download all the text for these books for free and use them to experiment with. [Project Gutenberg](https://www.gutenberg.org/) provides an extensive collection of books that are no longer under copyright.\n",
        "\n",
        "For this notebook, we will download the text for [Alice's Adventures in Wonderland](https://www.gutenberg.org/ebooks/11) by Lewis Carroll.\n",
        "\n",
        "Before we start building our model, we will need to prepare the text of this book so that we can easily work with it. The simplest way to do this is to [download the complete plain text (UTF-8) version of the book](https://www.gutenberg.org/cache/epub/11/pg11.txt) to your local machine. Save this version of the text with the filename `wonderland_raw.txt`.\n",
        "\n",
        "Project Gutenberg adds a header and footer information to each book, which is not part of the original text. Because we only want to train our model on the original text, open the file you have just saved in a text editor and delete the header and footer information.\n",
        "\n",
        "The header text ends with:\n",
        "\n",
        "> `*** START OF THIS PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***`\n",
        "\n",
        "The footer is all of the text after the line containing:\n",
        "\n",
        "> `THE END`\n",
        "\n",
        "You should be left with a text file that has a bit over 3,300 lines of text. Save this new version of the file as `wonderland_text.txt`. You will upload this file to this notebook to be the dataset to train your model.\n",
        "\n",
        "To upload the file, you should be able to open the *Files* panel on the left of this notebook and drag the `wonderland_text.txt` into the panel."
      ],
      "metadata": {
        "id": "hxrL_h6MblCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Small LSTM Network\n",
        "\n",
        "In this section, we will build a small LSTM network and train it to predict character sequences from *Alice in Wonderland*.\n",
        "\n",
        "We start by importing the libraries that we'll use to construct and train our model."
      ],
      "metadata": {
        "id": "7OQdn1dXep3b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsLzlJv9OjKz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we load the text file that we've uploaded into memory and convert all of the text to lowercase to reduce the vocabulary (of characters) that the network must learn."
      ],
      "metadata": {
        "id": "MIA5-PqgfIfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load ascii text and covert to lowercase\n",
        "filename = \"wonderland_text.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()"
      ],
      "metadata": {
        "id": "4zPhbANCOqUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to prepare the data by converting it into an appropriate form to input into a network. We don't model the characters directly, instead, we convert the characters into integer tokens.\n",
        "\n",
        "We do this by first creating a set of all of the different characters in the book, then creating a map (a `dict` in Python) of each character to a unique integer, by enumerating each character in the set and assigning it an integer equal to its index in the set. The particular value of the integers assigned to the chars is not important for the model."
      ],
      "metadata": {
        "id": "zGr4sbzOfaFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "metadata": {
        "id": "JrKVBScSP4RD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can examine the vocabulary of characters that have been extracted from the book by printing the `chars` set, which is a list of the unique lowercase letters, numbers, punctuation and special characters that appear in the text."
      ],
      "metadata": {
        "id": "Yd5ZzShkgMk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyiZJdLOP7g7",
        "outputId": "38b1b540-268d-4c21-c892-380cb71ee5f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '(', ')', '*', ',', '-', '.', '0', '3', ':', ';', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ù', '—', '‘', '’', '“', '”', '\\ufeff']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vocabulary still includes some characters that we might want to remove from the text, e.g., `*`, and this may improve the model by removing text we do not want to generate. But this will be sufficient for our purposes of working with an LSTM to generate some text.\n",
        "\n",
        "We can summarise the dataset in terms of the size of the whole text and the size of the vocabulary."
      ],
      "metadata": {
        "id": "b0aNNWPAhEwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: {}\".format(n_chars))\n",
        "print(\"Total Vocab: {}\".format(n_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSRfE3-jQHji",
        "outputId": "e986294c-1364-4c66-f4fe-6d71a6fa579b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters: 144581\n",
            "Total Vocab: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the text has a little under 145,000 characters in total and the vocabulary has around 46 characters. (Your values should be around these values, but you may have a slightly bigger or smaller vocabulary depending on how much you removed from the original raw file.)\n",
        "\n",
        "We now need to define the training data for the network. There is a lot of flexibility in how you choose to break up the text and present it to the network.\n",
        "\n",
        "In this tutorial, we will split the text into sequences with a fixed length of 100 characters. There is nothing particularly special about this length of sequence, it is an arbitrary length, but should be sufficient to demonstrate the use of an LSTM to generate text character-by-character.\n",
        "\n",
        "Another common approach to breaking up a dataset like this is to find the longest sentence in the text and use this as the sequence length. The text can then be broken into sentences, padding the shorter sequences such that the input to the network is of a uniform length.\n",
        "\n",
        "Each training pattern for the network comprises 100 time steps of one character (X) followed by one character output (y). When creating these sequences, we slide this window along the whole book one character at a time, allowing each character a chance to be learned from the 100 characters that preceded it, with the exception of the first 100 characters.\n",
        "\n",
        "For example, if the sequence length were just 5, then the first two training patterns would be as follows:\n",
        "\n",
        "> X: `CHAPT` -> y: `E`  \n",
        "> X: `HAPTE` -> y: `R`\n",
        "\n",
        "As we split the book into sequences, we also convert the characters to integers using the map we prepared earlier."
      ],
      "metadata": {
        "id": "k9sIIaKWh2HH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "  seq_in = raw_text[i:i + seq_length]\n",
        "  seq_out = raw_text[i + seq_length]\n",
        "  dataX.append([char_to_int[char] for char in seq_in])\n",
        "  dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: {}\".format(n_patterns))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dudn5r4XQtee",
        "outputId": "99aa33b9-d692-443c-df11-f4bc5ae58e8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Patterns: 144481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the code to this point shows that when we split up the dataset into training data for the network to learn that we have just under 145,000 training patterns.\n",
        "\n",
        "This makes sense because, excluding the first 100 characters, we have one training pattern to predict each of the remaining characters in the text."
      ],
      "metadata": {
        "id": "hfDSXEzxk6m5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have prepared the training data, we need to transform it again for use with Keras.\n",
        "\n",
        "First, we transform the list of input sequences into the form \\[samples, time steps, features\\], which is expected by an LSTM network.\n",
        "\n",
        "Next, we rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network using the sigmoid activation function by default.\n",
        "\n",
        "Finally, we convert the output patterns (single characters converted to integers) into a one-hot encoding. This is so that we can configure the network to predict the probability of each of the different characters in the vocabulary. Each `y` value is converted into a sparse vector with a length of the vocabulary, full of zeros, except with a single 1 in the column for the letter (integer) that the pattern represents.\n",
        "\n",
        "For example, when “n” (integer value 32) is one-hot encoded, it will look something like:\n",
        "\n",
        "> `[0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]`"
      ],
      "metadata": {
        "id": "5bK8-Kq5lYOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalise the inputs to be in the range 0..1\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the outputs\n",
        "y = to_categorical(dataY)"
      ],
      "metadata": {
        "id": "YsGzl4vOQ9PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now define the LSTM model. Here, we define a single hidden LSTM layer with 256 memory units. The network uses dropout with a probability of 20%. The output layer is a Dense layer using the softmax activation function to output a probability prediction (0..1) for each of the different characters.\n",
        "\n",
        "The problem is really a single character classification problem with the number of classes equal to the size of the vocabulary. As such, the model compiled as optimizing the  categorical crossentropy loss, using the ADAM optimisation algorithm."
      ],
      "metadata": {
        "id": "x7MSfvxSnEsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "qBl1-Uy7RDDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no test dataset in this example. We are modelling the entire training dataset to learn the probability of each character in a sequence. It can be useful to define a validation set, as we would normally for such a categorisation task, in particular to avoid overfitting on the given dataset. But for the sake of simplicity, this has been omitted from this example."
      ],
      "metadata": {
        "id": "GDgrguuhnqiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even a small LSTM network can be slow to train, because of this, we can use model checkpointing to record all the network weights to a file each time an improvement in the loss is observed at the end of the epoch. This will mean that we can use the best set of weights (lowest loss) to instantiate our generative model in the next section."
      ],
      "metadata": {
        "id": "GILr-5GVoK4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.keras\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "metadata": {
        "id": "1h2YfVTRRJE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now fit your model to the data. Here, we use a modest number of 20 epochs and a large batch size of 128 patterns, to ensure that we can get a result within a relatively short period of time."
      ],
      "metadata": {
        "id": "a9NWtR2bonDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRsZf6tyRM4e",
        "outputId": "b4548897-0daf-48a4-b4b6-8c883baf9b03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.0925\n",
            "Epoch 1: loss improved from inf to 3.01131, saving model to weights-improvement-01-3.0113.keras\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - loss: 3.0924\n",
            "Epoch 2/20\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.8442\n",
            "Epoch 2: loss improved from 3.01131 to 2.80871, saving model to weights-improvement-02-2.8087.keras\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - loss: 2.8442\n",
            "Epoch 3/20\n",
            "\u001b[1m1128/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.7222\n",
            "Epoch 3: loss improved from 2.80871 to 2.70271, saving model to weights-improvement-03-2.7027.keras\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - loss: 2.7222\n",
            "Epoch 4/20\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.6360\n",
            "Epoch 4: loss improved from 2.70271 to 2.62155, saving model to weights-improvement-04-2.6215.keras\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - loss: 2.6360\n",
            "Epoch 5/20\n",
            "\u001b[1m1126/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.5604\n",
            "Epoch 5: loss improved from 2.62155 to 2.55123, saving model to weights-improvement-05-2.5512.keras\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - loss: 2.5604\n",
            "Epoch 6/20\n",
            "\u001b[1m1126/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.4965\n",
            "Epoch 6: loss improved from 2.55123 to 2.49177, saving model to weights-improvement-06-2.4918.keras\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - loss: 2.4965\n",
            "Epoch 7/20\n",
            "\u001b[1m1128/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.4380\n",
            "Epoch 7: loss improved from 2.49177 to 2.43571, saving model to weights-improvement-07-2.4357.keras\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 2.4380\n",
            "Epoch 8/20\n",
            "\u001b[1m1126/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.3933\n",
            "Epoch 8: loss improved from 2.43571 to 2.38844, saving model to weights-improvement-08-2.3884.keras\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 2.3933\n",
            "Epoch 9/20\n",
            "\u001b[1m1128/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.3483\n",
            "Epoch 9: loss improved from 2.38844 to 2.34371, saving model to weights-improvement-09-2.3437.keras\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - loss: 2.3483\n",
            "Epoch 10/20\n",
            "\u001b[1m1127/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.3032\n",
            "Epoch 10: loss improved from 2.34371 to 2.30284, saving model to weights-improvement-10-2.3028.keras\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 2.3032\n",
            "Epoch 11/20\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.2645\n",
            "Epoch 11: loss improved from 2.30284 to 2.26480, saving model to weights-improvement-11-2.2648.keras\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - loss: 2.2645\n",
            "Epoch 12/20\n",
            "\u001b[1m1127/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.2250\n",
            "Epoch 12: loss improved from 2.26480 to 2.22488, saving model to weights-improvement-12-2.2249.keras\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - loss: 2.2250\n",
            "Epoch 13/20\n",
            "\u001b[1m1127/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.1895\n",
            "Epoch 13: loss improved from 2.22488 to 2.19207, saving model to weights-improvement-13-2.1921.keras\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 2.1895\n",
            "Epoch 14/20\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.1530\n",
            "Epoch 14: loss improved from 2.19207 to 2.15636, saving model to weights-improvement-14-2.1564.keras\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 2.1530\n",
            "Epoch 15/20\n",
            "\u001b[1m1128/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.1156\n",
            "Epoch 15: loss improved from 2.15636 to 2.12245, saving model to weights-improvement-15-2.1224.keras\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - loss: 2.1157\n",
            "Epoch 16/20\n",
            "\u001b[1m1127/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.0876\n",
            "Epoch 16: loss improved from 2.12245 to 2.09323, saving model to weights-improvement-16-2.0932.keras\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - loss: 2.0876\n",
            "Epoch 17/20\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.0596\n",
            "Epoch 17: loss improved from 2.09323 to 2.06133, saving model to weights-improvement-17-2.0613.keras\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 2.0596\n",
            "Epoch 18/20\n",
            "\u001b[1m1128/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.0230\n",
            "Epoch 18: loss improved from 2.06133 to 2.03377, saving model to weights-improvement-18-2.0338.keras\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - loss: 2.0231\n",
            "Epoch 19/20\n",
            "\u001b[1m1128/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.9963\n",
            "Epoch 19: loss improved from 2.03377 to 2.00656, saving model to weights-improvement-19-2.0066.keras\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - loss: 1.9963\n",
            "Epoch 20/20\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.9724\n",
            "Epoch 20: loss improved from 2.00656 to 1.98170, saving model to weights-improvement-20-1.9817.keras\n",
            "\u001b[1m1129/1129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - loss: 1.9725\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f7b75fcd000>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running the example, a number of weight checkpoint files should be visible in the local directory.\n",
        "\n",
        "All except the one with the smallest loss value can be deleted. The value of the loss is encoded in the filename of the checkpoint to make identifying the best checkpoint simple.\n",
        "\n",
        "If the network loss decreased every epoch, the model would likely benefit from additional training. But before doing that, let's have a look at what this network can generate."
      ],
      "metadata": {
        "id": "Aoc3ErP5o1Cy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Text\n",
        "\n",
        "Generating text using the trained LSTM network is relatively straightforward.\n",
        "\n",
        "First, we load the data and define the network in exactly the same way, except the network weights are loaded from a checkpoint file, and the network does not need to be re-trained. We do this because the last epoch of training may not have had the lowest loss. By reloading the weights form the checkpoint with the lowest loss we ensure that the model is the best it can be.\n",
        "\n",
        "**Note**: You will need to change the name of the checkpoint file in the code below to match the name of the file that has the lowest loss in your run."
      ],
      "metadata": {
        "id": "ZT1QK-0qSCap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the network weights\n",
        "filename = \"weights-improvement-20-2.0136.keras\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "iSbUlDp3SIP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To convert back from the integer tokens to the unique characters in the text, we create a reverse mapping."
      ],
      "metadata": {
        "id": "mU2ssfTJqVAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "metadata": {
        "id": "B0b5X_BwSNO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplest way to use LSTM model to make predictions is to first start with a seed sequence as input, generate the next character, then update the input to add the generated character on the end and trim off the first character. This process is repeated for as long as desired to predict new characters (e.g., a sequence of 1,000 characters in length).\n",
        "\n",
        "The following defines a function that does exactly that and outputs the generated characters as it goes. It picks a random sequence of characters from the dataset as its initial input pattern, or seed. (We will reuse this function with a larger model below, so the function takes the model to use to generate predictions as a parameter.)"
      ],
      "metadata": {
        "id": "nKsv6d5oqmPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# pick a random seed\n",
        "def generate_text(model, length=500): #Changed from 1000 to 500 to see if the model generalizes better\n",
        "  start = np.random.randint(0, len(dataX)-1)\n",
        "  pattern = dataX[start]\n",
        "  output = \"\"\n",
        "  print(\"Seed:\")\n",
        "  print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "  print(\"\\n\\nGenerated:\")\n",
        "  # generate characters\n",
        "  for i in range(length):\n",
        "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = np.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    output += result\n",
        "    sys.stdout.write(result)\n",
        "    sys.stdout.flush()\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "  print(\"\\n\\nDone\")"
      ],
      "metadata": {
        "id": "WYQKTgO9SSHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We call this function with the model we want it to use to generate the text."
      ],
      "metadata": {
        "id": "tX6O-zT9r_ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model)"
      ],
      "metadata": {
        "id": "lna4uoIkr9aT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2a4c0d1-8513-4a28-864f-bb891dbecce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "\" een to her,\n",
            "    and mentioned me to him:\n",
            "she gave me a good character,\n",
            "    but said i could not swim \"\n",
            "\n",
            "\n",
            "Generated:\n",
            "e and the taa itt it  a  atd toued oo to toee to teees the was shen a aatirslll as it  and taed to the hotthen  she was a gittll of the tine of the soeen  and sand to the horth  she was aeging in a lintte or the wfoh she thnte the had soeerid the shal she was she was soeeking and sooe the thele she had denn her hnad thtelker, and the was nol io an tele an the cadl  at the sasd thin  she was notting ano the wirte she pabbit sas she was she winte rabbit  ant torn it was ano tored to the whete sabb\n",
            "\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running this function first outputs the selected random seed, then each character as it is generated.\n",
        "\n",
        "**Note**: Your results will likely vary given the stochastic nature of the algorithm. Consider running the text generation function a few times and compare the average outcome.\n",
        "\n",
        "Some observations about the generated text:\n",
        "\n",
        "- Characters are separated into word-like groups, and some are actual English words, e.g., \"it\", \"to\", \"tea\", \"she\" \"more\", but many are not, e.g., \"hxiniin\", \"lirtle\", \"maae\", \"boeme\"\n",
        "- Occasionally, some of the words in a sequence make sense, e.g., \"and the white rabbit\" but many don't\n",
        "\n",
        "That a small LSTM network learning a character-based model can produce this output is impressive, although far from perfect. In the next section, we will look at improving the quality of the results by developing a larger LSTM network."
      ],
      "metadata": {
        "id": "6z6RawMMrtUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Bigger LSTM Network\n",
        "\n",
        "We got results, but not great results in the previous section. Now, we will try to improve the quality of the generated text by creating a larger network.\n",
        "\n",
        "The definition of the bigger model is not particularly complex, we will simply add a second LSTM layer, with dropout again set to 20% for the second layer. We keep the size of the LSTM layers the same at 256 units."
      ],
      "metadata": {
        "id": "aS-aZauxWyYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigger_model = Sequential()\n",
        "bigger_model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "bigger_model.add(Dropout(0.3)) #Increased dropout rate to make the model generalize more\n",
        "bigger_model.add(LSTM(256, return_sequences=True))\n",
        "bigger_model.add(Dropout(0.3))\n",
        "bigger_model.add(LSTM(256))\n",
        "bigger_model.add(Dropout(0.3))\n",
        "bigger_model.add(Dense(y.shape[1], activation='softmax'))\n",
        "bigger_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "bigger_model.summary()"
      ],
      "metadata": {
        "id": "R64SW96eW3Bg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "8751dd83-da41-41d5-a587-ae77e4090f48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m264,192\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m525,312\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m525,312\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m)                  │          \u001b[38;5;34m12,593\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">264,192</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">12,593</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,327,409\u001b[0m (5.06 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,327,409</span> (5.06 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,327,409\u001b[0m (5.06 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,327,409</span> (5.06 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll define a different filepath pattern for saving the checkpoints for this bigger model, so that we can tell them apart from the previous checkpoints. We'll also define a new checkpoint callback for this model, so that we aren't competing with the previous lowest loss found before any checkpoints will be saved."
      ],
      "metadata": {
        "id": "StPnK_NvuDDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigger_filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.keras\"\n",
        "bigger_checkpoint = ModelCheckpoint(bigger_filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "bigger_callbacks_list = [bigger_checkpoint]"
      ],
      "metadata": {
        "id": "QIGNl_4YW7Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we increase the number of training epochs from 20 to 50 and decrease the batch size from 128 to 64 to give the network more of an opportunity to be updated and learn. These changes will make the training slower but should result in a much more capable model."
      ],
      "metadata": {
        "id": "ZgCC0vyzujUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the model\n",
        "bigger_model.fit(X, y, epochs=70, batch_size=64, callbacks=bigger_callbacks_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JZH09vTW7_2",
        "outputId": "4d8e92c2-7a1f-43f9-f53c-0f19c5aec357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 3.1083\n",
            "Epoch 1: loss improved from inf to 3.08275, saving model to weights-improvement-01-3.0828-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 30ms/step - loss: 3.1083\n",
            "Epoch 2/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 3.0713\n",
            "Epoch 2: loss improved from 3.08275 to 3.06704, saving model to weights-improvement-02-3.0670-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 30ms/step - loss: 3.0713\n",
            "Epoch 3/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.9424\n",
            "Epoch 3: loss improved from 3.06704 to 2.88868, saving model to weights-improvement-03-2.8887-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 2.9424\n",
            "Epoch 4/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.6248\n",
            "Epoch 4: loss improved from 2.88868 to 2.54990, saving model to weights-improvement-04-2.5499-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 31ms/step - loss: 2.6247\n",
            "Epoch 5/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.3596\n",
            "Epoch 5: loss improved from 2.54990 to 2.31569, saving model to weights-improvement-05-2.3157-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 2.3596\n",
            "Epoch 6/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.1919\n",
            "Epoch 6: loss improved from 2.31569 to 2.16172, saving model to weights-improvement-06-2.1617-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 30ms/step - loss: 2.1919\n",
            "Epoch 7/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.0688\n",
            "Epoch 7: loss improved from 2.16172 to 2.04943, saving model to weights-improvement-07-2.0494-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 30ms/step - loss: 2.0688\n",
            "Epoch 8/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.9803\n",
            "Epoch 8: loss improved from 2.04943 to 1.96346, saving model to weights-improvement-08-1.9635-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 31ms/step - loss: 1.9802\n",
            "Epoch 9/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.8996\n",
            "Epoch 9: loss improved from 1.96346 to 1.89078, saving model to weights-improvement-09-1.8908-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 30ms/step - loss: 1.8996\n",
            "Epoch 10/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.8377\n",
            "Epoch 10: loss improved from 1.89078 to 1.83360, saving model to weights-improvement-10-1.8336-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.8377\n",
            "Epoch 11/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.7796\n",
            "Epoch 11: loss improved from 1.83360 to 1.78011, saving model to weights-improvement-11-1.7801-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 31ms/step - loss: 1.7796\n",
            "Epoch 12/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.7400\n",
            "Epoch 12: loss improved from 1.78011 to 1.73717, saving model to weights-improvement-12-1.7372-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 31ms/step - loss: 1.7400\n",
            "Epoch 13/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.6935\n",
            "Epoch 13: loss improved from 1.73717 to 1.69425, saving model to weights-improvement-13-1.6943-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 30ms/step - loss: 1.6935\n",
            "Epoch 14/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.6558\n",
            "Epoch 14: loss improved from 1.69425 to 1.66193, saving model to weights-improvement-14-1.6619-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 31ms/step - loss: 1.6558\n",
            "Epoch 15/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.6196\n",
            "Epoch 15: loss improved from 1.66193 to 1.62767, saving model to weights-improvement-15-1.6277-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 31ms/step - loss: 1.6196\n",
            "Epoch 16/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.5964\n",
            "Epoch 16: loss improved from 1.62767 to 1.59955, saving model to weights-improvement-16-1.5995-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 30ms/step - loss: 1.5964\n",
            "Epoch 17/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.5689\n",
            "Epoch 17: loss improved from 1.59955 to 1.57428, saving model to weights-improvement-17-1.5743-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 31ms/step - loss: 1.5689\n",
            "Epoch 18/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.5433\n",
            "Epoch 18: loss improved from 1.57428 to 1.55282, saving model to weights-improvement-18-1.5528-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 30ms/step - loss: 1.5433\n",
            "Epoch 19/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.5209\n",
            "Epoch 19: loss improved from 1.55282 to 1.52794, saving model to weights-improvement-19-1.5279-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.5209\n",
            "Epoch 20/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.5061\n",
            "Epoch 20: loss improved from 1.52794 to 1.50875, saving model to weights-improvement-20-1.5087-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.5061\n",
            "Epoch 21/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.4815\n",
            "Epoch 21: loss improved from 1.50875 to 1.48874, saving model to weights-improvement-21-1.4887-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.4815\n",
            "Epoch 22/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.4607\n",
            "Epoch 22: loss improved from 1.48874 to 1.47192, saving model to weights-improvement-22-1.4719-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 30ms/step - loss: 1.4607\n",
            "Epoch 23/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.4470\n",
            "Epoch 23: loss improved from 1.47192 to 1.45474, saving model to weights-improvement-23-1.4547-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 30ms/step - loss: 1.4470\n",
            "Epoch 24/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.4329\n",
            "Epoch 24: loss improved from 1.45474 to 1.44081, saving model to weights-improvement-24-1.4408-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.4329\n",
            "Epoch 25/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.4079\n",
            "Epoch 25: loss improved from 1.44081 to 1.42302, saving model to weights-improvement-25-1.4230-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.4079\n",
            "Epoch 26/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.3983\n",
            "Epoch 26: loss improved from 1.42302 to 1.41098, saving model to weights-improvement-26-1.4110-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 30ms/step - loss: 1.3983\n",
            "Epoch 27/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.3891\n",
            "Epoch 27: loss improved from 1.41098 to 1.39998, saving model to weights-improvement-27-1.4000-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 30ms/step - loss: 1.3891\n",
            "Epoch 28/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.3759\n",
            "Epoch 28: loss improved from 1.39998 to 1.38828, saving model to weights-improvement-28-1.3883-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.3759\n",
            "Epoch 29/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.3611\n",
            "Epoch 29: loss improved from 1.38828 to 1.37532, saving model to weights-improvement-29-1.3753-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.3611\n",
            "Epoch 30/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1.3553\n",
            "Epoch 30: loss improved from 1.37532 to 1.36359, saving model to weights-improvement-30-1.3636-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 29ms/step - loss: 1.3553\n",
            "Epoch 31/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.3363\n",
            "Epoch 31: loss improved from 1.36359 to 1.35287, saving model to weights-improvement-31-1.3529-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.3363\n",
            "Epoch 32/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1.3414\n",
            "Epoch 32: loss improved from 1.35287 to 1.34566, saving model to weights-improvement-32-1.3457-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 29ms/step - loss: 1.3414\n",
            "Epoch 33/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.3184\n",
            "Epoch 33: loss improved from 1.34566 to 1.33172, saving model to weights-improvement-33-1.3317-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 30ms/step - loss: 1.3184\n",
            "Epoch 34/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1.3136\n",
            "Epoch 34: loss improved from 1.33172 to 1.32793, saving model to weights-improvement-34-1.3279-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 29ms/step - loss: 1.3136\n",
            "Epoch 35/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.3062\n",
            "Epoch 35: loss improved from 1.32793 to 1.31561, saving model to weights-improvement-35-1.3156-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.3062\n",
            "Epoch 36/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1.3007\n",
            "Epoch 36: loss improved from 1.31561 to 1.30829, saving model to weights-improvement-36-1.3083-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 29ms/step - loss: 1.3007\n",
            "Epoch 37/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.2934\n",
            "Epoch 37: loss improved from 1.30829 to 1.30406, saving model to weights-improvement-37-1.3041-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 30ms/step - loss: 1.2934\n",
            "Epoch 38/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.2868\n",
            "Epoch 38: loss improved from 1.30406 to 1.29006, saving model to weights-improvement-38-1.2901-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.2868\n",
            "Epoch 39/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.2803\n",
            "Epoch 39: loss improved from 1.29006 to 1.28602, saving model to weights-improvement-39-1.2860-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.2803\n",
            "Epoch 40/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.2611\n",
            "Epoch 40: loss improved from 1.28602 to 1.27663, saving model to weights-improvement-40-1.2766-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.2611\n",
            "Epoch 41/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.2599\n",
            "Epoch 41: loss improved from 1.27663 to 1.27407, saving model to weights-improvement-41-1.2741-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.2599\n",
            "Epoch 42/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.2527\n",
            "Epoch 42: loss improved from 1.27407 to 1.26509, saving model to weights-improvement-42-1.2651-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.2527\n",
            "Epoch 43/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.2464\n",
            "Epoch 43: loss improved from 1.26509 to 1.26083, saving model to weights-improvement-43-1.2608-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.2464\n",
            "Epoch 44/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.2406\n",
            "Epoch 44: loss improved from 1.26083 to 1.25458, saving model to weights-improvement-44-1.2546-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.2406\n",
            "Epoch 45/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.2394\n",
            "Epoch 45: loss improved from 1.25458 to 1.24801, saving model to weights-improvement-45-1.2480-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.2394\n",
            "Epoch 46/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.2260\n",
            "Epoch 46: loss improved from 1.24801 to 1.24399, saving model to weights-improvement-46-1.2440-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.2261\n",
            "Epoch 47/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.2314\n",
            "Epoch 47: loss improved from 1.24399 to 1.23828, saving model to weights-improvement-47-1.2383-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.2314\n",
            "Epoch 48/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.2241\n",
            "Epoch 48: loss improved from 1.23828 to 1.23343, saving model to weights-improvement-48-1.2334-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.2242\n",
            "Epoch 49/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.2159\n",
            "Epoch 49: loss improved from 1.23343 to 1.23077, saving model to weights-improvement-49-1.2308-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.2159\n",
            "Epoch 50/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.2116\n",
            "Epoch 50: loss improved from 1.23077 to 1.22081, saving model to weights-improvement-50-1.2208-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.2116\n",
            "Epoch 51/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.2023\n",
            "Epoch 51: loss improved from 1.22081 to 1.21977, saving model to weights-improvement-51-1.2198-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 30ms/step - loss: 1.2024\n",
            "Epoch 52/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.1964\n",
            "Epoch 52: loss improved from 1.21977 to 1.21037, saving model to weights-improvement-52-1.2104-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.1964\n",
            "Epoch 53/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.2004\n",
            "Epoch 53: loss did not improve from 1.21037\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 30ms/step - loss: 1.2004\n",
            "Epoch 54/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.1914\n",
            "Epoch 54: loss improved from 1.21037 to 1.20212, saving model to weights-improvement-54-1.2021-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.1914\n",
            "Epoch 55/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.1949\n",
            "Epoch 55: loss did not improve from 1.20212\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.1949\n",
            "Epoch 56/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.1889\n",
            "Epoch 56: loss improved from 1.20212 to 1.19552, saving model to weights-improvement-56-1.1955-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 30ms/step - loss: 1.1889\n",
            "Epoch 57/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.1814\n",
            "Epoch 57: loss improved from 1.19552 to 1.19479, saving model to weights-improvement-57-1.1948-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 30ms/step - loss: 1.1814\n",
            "Epoch 58/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.1763\n",
            "Epoch 58: loss improved from 1.19479 to 1.18865, saving model to weights-improvement-58-1.1887-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 30ms/step - loss: 1.1763\n",
            "Epoch 59/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.1789\n",
            "Epoch 59: loss did not improve from 1.18865\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.1789\n",
            "Epoch 60/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.1735\n",
            "Epoch 60: loss improved from 1.18865 to 1.18374, saving model to weights-improvement-60-1.1837-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.1735\n",
            "Epoch 61/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.1678\n",
            "Epoch 61: loss improved from 1.18374 to 1.18310, saving model to weights-improvement-61-1.1831-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.1678\n",
            "Epoch 62/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.1686\n",
            "Epoch 62: loss improved from 1.18310 to 1.17604, saving model to weights-improvement-62-1.1760-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.1686\n",
            "Epoch 63/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.1600\n",
            "Epoch 63: loss improved from 1.17604 to 1.17185, saving model to weights-improvement-63-1.1719-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.1600\n",
            "Epoch 64/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.1600\n",
            "Epoch 64: loss improved from 1.17185 to 1.16913, saving model to weights-improvement-64-1.1691-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.1601\n",
            "Epoch 65/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.1492\n",
            "Epoch 65: loss improved from 1.16913 to 1.16374, saving model to weights-improvement-65-1.1637-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.1492\n",
            "Epoch 66/70\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.1581\n",
            "Epoch 66: loss did not improve from 1.16374\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.1581\n",
            "Epoch 67/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.1521\n",
            "Epoch 67: loss improved from 1.16374 to 1.15879, saving model to weights-improvement-67-1.1588-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.1521\n",
            "Epoch 68/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.1566\n",
            "Epoch 68: loss improved from 1.15879 to 1.15876, saving model to weights-improvement-68-1.1588-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.1566\n",
            "Epoch 69/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.1449\n",
            "Epoch 69: loss improved from 1.15876 to 1.15367, saving model to weights-improvement-69-1.1537-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 30ms/step - loss: 1.1449\n",
            "Epoch 70/70\n",
            "\u001b[1m2257/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.1462\n",
            "Epoch 70: loss improved from 1.15367 to 1.14896, saving model to weights-improvement-70-1.1490-bigger.keras\n",
            "\u001b[1m2258/2258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.1462\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f7b642c0910>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running this bigger model, you can expect to achieve a loss of about 1.2. This should be significantly less than the loss achieved for the original model, which would often plateau around 1.6.\n",
        "\n",
        "Before proceeding, make sure that you load the checkpoint for the lowest loss, in case this wasn't the final epoch."
      ],
      "metadata": {
        "id": "C0pN4hiDuusp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the network weights\n",
        "bigger_filename = \"weights-improvement-70-1.1490-bigger.keras\"\n",
        "bigger_model.load_weights(bigger_filename)\n",
        "bigger_model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "RAaxNhM2vP_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f5a4ef8-babd-4d1c-cc49-688be8bab2a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 24 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now run the function that we defined earlier with the bigger model, to generate some text."
      ],
      "metadata": {
        "id": "I5UWdV4gvn-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(bigger_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMSh6zyeksKY",
        "outputId": "31c82fc2-eabc-4e53-e731-071955915c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "\" one, indeed!” said the dormouse indignantly. however, he consented to\n",
            "go on. “and so these three lit \"\n",
            "\n",
            "\n",
            "Generated:\n",
            "tle shings ” \n",
            "“i don’t know what i shall be all the rame tay that is the same thing and seemed to think that is is to be a little bett of the way of the sabbit way off to the same to see it to the sabbit, and the pueen surprised to her to her angrily. \n",
            "“you may not a dat was the best was a little be a little bertain ” said the king. \n",
            "“then it might any mind that say of a cat of the sea,” and she went on again: \n",
            "“the pueen was the breatures of the sea,” said the king.\n",
            "\n",
            "“then i should think that i\n",
            "\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there are generally fewer spelling mistakes, and the text looks more realistic but is still quite nonsensical.\n",
        "\n",
        "For example, the same phrases get repeated again and again, e.g., \"will you\" and \"won't you\". Quotes are opened but not closed.\n",
        "\n",
        "The results of this model are significantly better than the previous model, but there is still a lot of room for improvement."
      ],
      "metadata": {
        "id": "27wFNF-0vvWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improving the Model\n",
        "\n",
        "Here are some ideas for improving the model that you might want to try:\n",
        "\n",
        "- Predict fewer than 1,000 characters as output for a given seed (the LSTM performs better the closer it is to the seed, so shorter sequences should be more coherent)\n",
        "- Clean the source text more thoroughly, e.g., remove all punctuation from the source text and, therefore, from the models’ vocabulary\n",
        "- Try a different source text; Alice in Wonderland is a brilliant book but Lewis Carroll was a master of non-sensical rhymes and it may be easier to judge the success of the model on a different source\n",
        "- Increase the number of training epochs, you can easily do this by loading a checkpoint and continuing the training from there\n",
        "- Try changing the dropout percentage to see if this has a noticeable impact on the generated output\n",
        "- Try changing the batch size, e.g., start with a batch size of 1 and slowly increase the batch size to see if you can find one that performs better\n",
        "- Add more memory units and/or layers to the model\n",
        "\n",
        "## Tutorial Assignment\n",
        "\n",
        "Attempt at least 2 of the above suggestions (or some changes of your own devising, e.g., you might want to replace the LSTM layers with GRU layers and see how this impacts speed of training and performance on your dataset). Report on your experiments by submitting your notebook with comments on the things that you tried, what worked better than expected, what worked worse."
      ],
      "metadata": {
        "id": "RW2FswKOwUJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment 3\n",
        "\n",
        "Volodymyr Kalinin, s3648214\n",
        "\n",
        "For this assignment, I have experimented with the output length, number of epochs, and the dropout percentage to see if the model would generalize better.\n",
        "\n",
        "Assumption before experiment: The model will generalize better, however, may underfit if the dropout I have set is too high (0.2 -> 0.3). I am hoping to account for this by having a lower output text size, since LSTMs struggle with long-term dependancies, so that whole text will be coherent (this may not fix the issue, however, since the generated text may look stockastic already from the start). Additionally, I will increase the epoch number by 20, just to be sure the model is not underfitted.\n",
        "\n",
        "Results after experiment: The assumption was correct, the words became more diverse, with slight alterations from the overall tone and style of Alice in Wonderland. The higher number of epochs definitely lessened the amount of stochasity, however the 10% increase to the dropout rate was a bit too much, since more words became incorrect (rame, tay, dat). 25% would have influenced the model better. The lesser output length has also improved the overall quality of the text by limiting the model to not overgenerate the words."
      ],
      "metadata": {
        "id": "7Ggtsqqig30m"
      }
    }
  ]
}